import asyncio
import time
import math
import random
from datetime import datetime
import numpy as np
import pandas as pd
from sklearn.ensemble import GradientBoostingRegressor, IsolationForest
from sklearn.metrics import mean_absolute_error
import logging

# ---------------- Basic setup ----------------
logging.basicConfig(
Â    level=logging.INFO,
Â    format="%(asctime)s | %(levelname)s | %(message)s",
Â    datefmt="%H:%M:%S"
)

# ---------------- Configurations ----------------
class Config:
Â    # These are some tuning values for the autoscaler
Â    TARGET_RPS_PER_REPLICA = 50  # each replica can handle this much RPS
Â    MIN_REPLICAS = 1
Â    MAX_REPLICAS = 20
Â    SCALE_UP_STEP = 3
Â    SCALE_DOWN_STEP = 2
Â    SCALE_UP_COOLDOWN = 60       # seconds
Â    SCALE_DOWN_COOLDOWN = 120
Â    ANOMALY_WINDOW = 60          # how much data to check for anomaly
Â    SAMPLE_FREQ = 1              # 1 minute
Â    PRED_HORIZON = 3             # predict next 3 minutes


# ---------------- Monitor ----------------
class Monitor:
Â    """This class just makes some fake data like RPS (Requests per second)."""
Â    def __init__(self, seed=42):
Â        random.seed(seed)
Â        self.data = self._generate_fake_data(24 * 60)  # 1 day = 1440 mins

Â    def _generate_fake_data(self, minutes):
Â        # make some sine pattern like day-night usage pattern
Â        t = np.arange(minutes)
Â        pattern = 40 + 25 * np.sin(2 * np.pi * t / (24 * 60))
Â        noise = np.random.normal(0, 4, size=minutes)

Â        spikes = np.zeros_like(t, dtype=float)
Â        for _ in range(6):  # add few random spikes
Â            c = random.randint(0, minutes - 1)
Â            spikes += random.randint(30, 70) * np.exp(-0.5 * ((t - c) / 4) ** 2)

Â        rps = np.clip(pattern + noise + spikes, 0, None)
Â        df = pd.DataFrame({"rps": rps})
Â        df.index = pd.date_range(end=pd.Timestamp.now(), periods=minutes, freq="1min")
Â        return df

Â    async def collect(self):
Â        """Returns latest few RPS values."""
Â        await asyncio.sleep(0.1)
Â        latest = self.data.tail(5)["rps"]
Â        avg_rps = float(latest.mean())
Â        print(f"[Monitor] Current Avg RPS: {avg_rps:.2f}")
Â        return {
Â            "timestamp": latest.index[-1],
Â            "rps": avg_rps,
Â            "raw_series": latest
Â        }


# ---------------- Forecaster ----------------
class Forecaster:
Â    """This will predict what might happen next (next few RPS values)."""
Â    def __init__(self):
Â        self.model = GradientBoostingRegressor(random_state=42)

Â    def train(self, series: pd.Series):
Â        df = pd.DataFrame({"y": series})
Â        # creating lag features
Â        df["lag1"] = df["y"].shift(1)
Â        df["lag2"] = df["y"].shift(2)
Â        df["lag3"] = df["y"].shift(3)
Â        df["target"] = df["y"].shift(-1)
Â        df = df.dropna()

Â        X = df[["lag1", "lag2", "lag3"]]
Â        y = df["target"]

Â        split = int(len(X) * 0.8)
Â        X_train, X_test = X.iloc[:split], X.iloc[split:]
Â        y_train, y_test = y.iloc[:split], y.iloc[split:]

Â        self.model.fit(X_train, y_train)
Â        preds = self.model.predict(X_test)
Â        mae = mean_absolute_error(y_test, preds)
Â        print(f"[Forecaster] Model trained successfully. MAE = {mae:.2f}")

Â    def predict(self, series: pd.Series):
Â        preds = []
Â        s = list(series[-3:])
Â        for _ in range(Config.PRED_HORIZON):
Â            X = np.array(s[-3:]).reshape(1, -1)
Â            next_val = float(self.model.predict(X)[0])
Â            preds.append(next_val)
Â            s.append(next_val)
Â        print(f"[Forecaster] Next {Config.PRED_HORIZON} min prediction: {preds}")
Â        return preds


# ---------------- Anomaly Detector ----------------
class AnomalyDetector:
Â    """This checks if a sudden spike or drop has happened."""
Â    def __init__(self):
Â        self.detector = IsolationForest(contamination=0.01, random_state=42)

Â    def fit(self, series: pd.Series):
Â        self.detector.fit(series.values.reshape(-1, 1))
Â        print("[AnomalyDetector] Trained on recent data")

Â    def is_anomaly(self, value: float) -> bool:
Â        pred = self.detector.predict(np.array([[value]]))[0]
Â        if pred == -1:
Â            print(f"[AnomalyDetector] ðŸš¨ Anomaly Detected! Value = {value}")
Â            return True
Â        return False


# ---------------- Decision Engine ----------------
class DecisionEngine:
Â    """Takes decisions based on forecast and anomaly."""
Â    def __init__(self):
Â        self.current_replicas = 3
Â        self.last_scale_time = 0

Â    def decide(self, current_rps, forecast, anomaly):
Â        target_per_replica = Config.TARGET_RPS_PER_REPLICA
Â        predicted = forecast[0] if forecast else current_rps
Â        desired = math.ceil(predicted / target_per_replica)
Â        desired = max(Config.MIN_REPLICAS, min(Config.MAX_REPLICAS, desired))

Â        now = time.time()

Â        # Cooldown check to prevent over-scaling
Â        if desired > self.current_replicas and (now - self.last_scale_time < Config.SCALE_UP_COOLDOWN):
Â            return self.current_replicas, "cooldown_up"
Â        if desired < self.current_replicas and (now - self.last_scale_time < Config.SCALE_DOWN_COOLDOWN):
Â            return self.current_replicas, "cooldown_down"

Â        # Slowly scale up or down
Â        if desired > self.current_replicas:
Â            new_replicas = self.current_replicas + Config.SCALE_UP_STEP
Â            reason = "scale_up"
Â        elif desired < self.current_replicas:
Â            new_replicas = max(Config.MIN_REPLICAS, self.current_replicas - Config.SCALE_DOWN_STEP)
Â            reason = "scale_down"
Â        else:
Â            new_replicas = self.current_replicas
Â            reason = "no_change"

Â        # If sudden spike detected, add one extra replica
Â        if anomaly and new_replicas == self.current_replicas:
Â            new_replicas += 1
Â            reason = "spike_detected"

Â        # Update scale time and replicas
Â        if new_replicas != self.current_replicas:
Â            self.last_scale_time = now
Â            self.current_replicas = new_replicas

Â        print(f"[DecisionEngine] {reason.upper()} â†’ New replica count: {new_replicas}")
Â        return new_replicas, reason


# ---------------- Executor ----------------
class Executor:
Â    """This is like the system applying the scaling change."""
Â    async def apply(self, from_count, to_count, reason):
Â        await asyncio.sleep(0.2)
Â        print(f"[Executor] Scaling from {from_count} â†’ {to_count} because of {reason}")


# ---------------- AutoScaler Orchestrator ----------------
class AutoScaler:
Â    """Main class which connects all the parts together."""
Â    def __init__(self):
Â        self.monitor = Monitor()
Â        self.forecaster = Forecaster()
Â        self.detector = AnomalyDetector()
Â        self.decision = DecisionEngine()
Â        self.executor = Executor()

Â        # train model and detector
Â        self.forecaster.train(self.monitor.data["rps"])
Â        self.detector.fit(self.monitor.data.tail(Config.ANOMALY_WINDOW)["rps"])

Â    async def run_cycle(self):
Â        metrics = await self.monitor.collect()
Â        rps = metrics["rps"]
Â        forecast = self.forecaster.predict(metrics["raw_series"])
Â        anomaly = self.detector.is_anomaly(rps)

Â        new_count, reason = self.decision.decide(rps, forecast, anomaly)
Â        await self.executor.apply(self.decision.current_replicas, new_count, reason)

Â    async def run(self, cycles=10):
Â        print("[AutoScaler] Starting demo run...")
Â        for i in range(cycles):
Â            print(f"\n[Cycle {i+1}] ----------------------------")
Â            await self.run_cycle()
Â            await asyncio.sleep(2)
Â        print("[AutoScaler] Demo completed!")


# ---------------- Main ----------------
if __name__ == "__main__":
Â    asyncio.run(AutoScaler().run(cycles=10))